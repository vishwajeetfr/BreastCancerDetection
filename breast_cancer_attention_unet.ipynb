{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection Using Advanced Deep Learning\n",
    "\n",
    "## Personal Motivation\n",
    "This project represents my exploration into combining attention mechanisms with U-Net architecture \n",
    "for improved medical image segmentation.\n",
    "\n",
    "## My Technical Approach\n",
    "- Implementation of a novel Attention-Enhanced U-Net architecture\n",
    "- Custom preprocessing pipeline for medical ultrasound data\n",
    "- Advanced visualization techniques for model interpretability\n",
    "- Comprehensive evaluation framework with multiple metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_3rM2Af6CwS"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:34.48354Z",
     "iopub.status.busy": "2022-09-05T03:01:34.482836Z",
     "iopub.status.idle": "2022-09-05T03:01:46.062539Z",
     "shell.execute_reply": "2022-09-05T03:01:46.061242Z",
     "shell.execute_reply.started": "2022-09-05T03:01:34.483427Z"
    },
    "id": "ni0qxGc3L1bu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install tf_explain\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:46.065613Z",
     "iopub.status.busy": "2022-09-05T03:01:46.064835Z",
     "iopub.status.idle": "2022-09-05T03:01:51.8092Z",
     "shell.execute_reply": "2022-09-05T03:01:51.808145Z",
     "shell.execute_reply.started": "2022-09-05T03:01:46.065561Z"
    },
    "id": "4zXw8Ycj6D0l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# common\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.image as tfi\n",
    "\n",
    "# Data\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Data Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model \n",
    "from keras.models import Model\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Add\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Callbacks \n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "# Metrics\n",
    "from keras.metrics import MeanIoU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zckEunbDxLy"
   },
   "source": [
    "# **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:51.812416Z",
     "iopub.status.busy": "2022-09-05T03:01:51.811434Z",
     "iopub.status.idle": "2022-09-05T03:01:51.820192Z",
     "shell.execute_reply": "2022-09-05T03:01:51.819217Z",
     "shell.execute_reply.started": "2022-09-05T03:01:51.812376Z"
    },
    "id": "Sv5MB-unO3tY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, size):\n",
    "    \"\"\"\n",
    "    Loads a single image from disk and rescales it to the given size.\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        size (int): The width and height to resize the image to.\n",
    "    Returns:\n",
    "        np.ndarray: The resized and normalized image as a numpy array.\n",
    "    \"\"\"\n",
    "    image = load_img(image_path)\n",
    "    arr = img_to_array(image) / 255.0\n",
    "    return np.round(tfi.resize(arr, (size, size)), 4)\n",
    "\n",
    "def load_images(image_paths, size, mask=False, limit=None):\n",
    "    \"\"\"\n",
    "    Loads and processes multiple images from the provided paths.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of image file paths.\n",
    "        size (int): Image resize target.\n",
    "        mask (bool): Whether to load as single-channel mask.\n",
    "        limit (int): Optional limit on number of images for quick experiments.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Batch of resized (and possibly single channel) images.\n",
    "    \"\"\"\n",
    "    paths = image_paths[:limit] if limit is not None else image_paths\n",
    "    if mask:\n",
    "        images = np.zeros((len(paths), size, size, 1))\n",
    "    else:\n",
    "        images = np.zeros((len(paths), size, size, 3))\n",
    "    for i, path in enumerate(paths):\n",
    "        img = load_image(path, size)\n",
    "        images[i] = img[..., :1] if mask else img\n",
    "    return images\n",
    "\n",
    "def show_image(image, title=None, cmap=None, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Utility to display a single image using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        image: Image array.\n",
    "        title (str): Title for the plot.\n",
    "        cmap: Matplotlib color map.\n",
    "        alpha (float): Image transparency.\n",
    "    \"\"\"\n",
    "    plt.imshow(image, cmap=cmap, alpha=alpha)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def show_mask(image, mask, cmap=None, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Overlays a mask on an image.\n",
    "\n",
    "    Args:\n",
    "        image: Base image.\n",
    "        mask: Segmentation mask.\n",
    "        cmap: Matplotlib color map.\n",
    "        alpha: Mask opacity.\n",
    "    \"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(tf.squeeze(mask), cmap=cmap, alpha=alpha)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:51.843276Z",
     "iopub.status.busy": "2022-09-05T03:01:51.842062Z",
     "iopub.status.idle": "2022-09-05T03:01:51.849482Z",
     "shell.execute_reply": "2022-09-05T03:01:51.848375Z",
     "shell.execute_reply.started": "2022-09-05T03:01:51.843237Z"
    },
    "id": "1BVzD2xgFkgt",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:51.851351Z",
     "iopub.status.busy": "2022-09-05T03:01:51.850897Z",
     "iopub.status.idle": "2022-09-05T03:01:51.870184Z",
     "shell.execute_reply": "2022-09-05T03:01:51.869055Z",
     "shell.execute_reply.started": "2022-09-05T03:01:51.851316Z"
    },
    "id": "wP8jF-PHO9Ch",
    "outputId": "f8dc581b-0a21-494b-8a74-73ce6fe5b9a3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "root_path = '../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/'\n",
    "classes = sorted(os.listdir(root_path))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:51.872095Z",
     "iopub.status.busy": "2022-09-05T03:01:51.871752Z",
     "iopub.status.idle": "2022-09-05T03:01:52.237175Z",
     "shell.execute_reply": "2022-09-05T03:01:52.236207Z",
     "shell.execute_reply.started": "2022-09-05T03:01:51.872062Z"
    },
    "id": "gVbNZJujO8_K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "single_mask_paths = sorted([sorted(glob(root_path + name + \"/*mask.png\")) for name in classes])\n",
    "double_mask_paths = sorted([sorted(glob(root_path + name + \"/*mask_1.png\")) for name in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:52.240195Z",
     "iopub.status.busy": "2022-09-05T03:01:52.239503Z",
     "iopub.status.idle": "2022-09-05T03:01:52.24625Z",
     "shell.execute_reply": "2022-09-05T03:01:52.245286Z",
     "shell.execute_reply.started": "2022-09-05T03:01:52.240157Z"
    },
    "id": "hizq3EyITaeI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "mask_paths = []\n",
    "for class_path in single_mask_paths:\n",
    "    for path in class_path:\n",
    "        img_path = path.replace('_mask','')\n",
    "        image_paths.append(img_path)\n",
    "        mask_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:52.248193Z",
     "iopub.status.busy": "2022-09-05T03:01:52.247792Z",
     "iopub.status.idle": "2022-09-05T03:01:55.27378Z",
     "shell.execute_reply": "2022-09-05T03:01:55.272487Z",
     "shell.execute_reply.started": "2022-09-05T03:01:52.248159Z"
    },
    "id": "RYwn8wnqTpa4",
    "outputId": "35a2e47f-35bd-4cf8-f0c0-a1ccbbc78396",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image(image_paths[0], SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:55.284839Z",
     "iopub.status.busy": "2022-09-05T03:01:55.280056Z",
     "iopub.status.idle": "2022-09-05T03:01:55.639848Z",
     "shell.execute_reply": "2022-09-05T03:01:55.638815Z",
     "shell.execute_reply.started": "2022-09-05T03:01:55.284784Z"
    },
    "id": "1G1w3V66UyPC",
    "outputId": "dd8acb8a-0f2d-4885-fd7a-052e0a36044a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_mask(load_image(image_paths[0], SIZE), load_image(mask_paths[0], SIZE)[:,:,0], alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIjk4Ps8RDvW"
   },
   "source": [
    "## **Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KgLQ53URF5X"
   },
   "source": [
    "Below here I have explained my strategy to tackel the multiple mask Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:55.64203Z",
     "iopub.status.busy": "2022-09-05T03:01:55.641121Z",
     "iopub.status.idle": "2022-09-05T03:01:55.784943Z",
     "shell.execute_reply": "2022-09-05T03:01:55.783613Z",
     "shell.execute_reply.started": "2022-09-05T03:01:55.641981Z"
    },
    "id": "64r6IQglO881",
    "outputId": "0746c333-d114-4e2e-c8c6-99260f5ea934",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:55.78724Z",
     "iopub.status.busy": "2022-09-05T03:01:55.786867Z",
     "iopub.status.idle": "2022-09-05T03:01:55.911419Z",
     "shell.execute_reply": "2022-09-05T03:01:55.910011Z",
     "shell.execute_reply.started": "2022-09-05T03:01:55.787203Z"
    },
    "id": "RS3v3F9wO86k",
    "outputId": "8344b3b9-2fb3-4c7f-fed2-37420ff66b6a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100)_mask_1.png', SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:55.913943Z",
     "iopub.status.busy": "2022-09-05T03:01:55.913537Z",
     "iopub.status.idle": "2022-09-05T03:01:56.050891Z",
     "shell.execute_reply": "2022-09-05T03:01:56.049403Z",
     "shell.execute_reply.started": "2022-09-05T03:01:55.913905Z"
    },
    "id": "J7IujS3sPhst",
    "outputId": "7e6a05b7-9039-423a-c8f1-db709be2c762",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100)_mask.png', SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68dZSoJ6PkSf"
   },
   "source": [
    "I don't want the data this way, as both the masks belongs to the same class. A better idea can be to merge both these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:56.053129Z",
     "iopub.status.busy": "2022-09-05T03:01:56.052785Z",
     "iopub.status.idle": "2022-09-05T03:01:56.185001Z",
     "shell.execute_reply": "2022-09-05T03:01:56.183861Z",
     "shell.execute_reply.started": "2022-09-05T03:01:56.053095Z"
    },
    "id": "FhVS-PhZPhqP",
    "outputId": "8a8ed1e8-32c3-424b-d762-c4f597c4297c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img = np.zeros((1,SIZE,SIZE,3))\n",
    "mask1 = load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100)_mask_1.png', SIZE)\n",
    "mask2 = load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100)_mask.png', SIZE)\n",
    "\n",
    "img = img + mask1 + mask2\n",
    "img = img[0,:,:,0]\n",
    "show_image(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O942E22FRNP3"
   },
   "source": [
    "We first merged them and them simple used the 1st channel because that is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:56.18767Z",
     "iopub.status.busy": "2022-09-05T03:01:56.186807Z",
     "iopub.status.idle": "2022-09-05T03:01:56.360032Z",
     "shell.execute_reply": "2022-09-05T03:01:56.359079Z",
     "shell.execute_reply.started": "2022-09-05T03:01:56.187632Z"
    },
    "id": "YHPoLXTFP4C6",
    "outputId": "90776d7f-a6e2-45f3-c7c9-197e1ff2873a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))\n",
    "plt.imshow(img, cmap='binary', alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:56.362513Z",
     "iopub.status.busy": "2022-09-05T03:01:56.361822Z",
     "iopub.status.idle": "2022-09-05T03:01:56.518954Z",
     "shell.execute_reply": "2022-09-05T03:01:56.51779Z",
     "shell.execute_reply.started": "2022-09-05T03:01:56.36246Z"
    },
    "id": "On868uBCP5Jj",
    "outputId": "24cc41be-cc61-42f7-dcd2-525ee6289f92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))\n",
    "plt.imshow(img, cmap='gray', alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:56.526191Z",
     "iopub.status.busy": "2022-09-05T03:01:56.523975Z",
     "iopub.status.idle": "2022-09-05T03:01:56.693508Z",
     "shell.execute_reply": "2022-09-05T03:01:56.692534Z",
     "shell.execute_reply.started": "2022-09-05T03:01:56.526153Z"
    },
    "id": "gXt6YcqKP5Gu",
    "outputId": "19afe93f-e15b-4065-bbdc-90d4583ff33c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_image(load_image('../input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT/benign/benign (100).png', SIZE))\n",
    "plt.imshow(img, alpha=0.4)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGkJpMajRXuK"
   },
   "source": [
    "This is how it looks with different cmaps. But you can drop them as then are very less in number (i.e 16) and this will not affect training much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PR-k_kiIRc5o"
   },
   "source": [
    "## **Data Work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:01:56.695622Z",
     "iopub.status.busy": "2022-09-05T03:01:56.69498Z",
     "iopub.status.idle": "2022-09-05T03:02:21.829549Z",
     "shell.execute_reply": "2022-09-05T03:02:21.828515Z",
     "shell.execute_reply.started": "2022-09-05T03:01:56.695572Z"
    },
    "id": "NuDIR8PFP5EW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = load_images(image_paths, SIZE)\n",
    "masks = load_images(mask_paths, SIZE, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:21.83152Z",
     "iopub.status.busy": "2022-09-05T03:02:21.83113Z",
     "iopub.status.idle": "2022-09-05T03:02:22.995772Z",
     "shell.execute_reply": "2022-09-05T03:02:22.994916Z",
     "shell.execute_reply.started": "2022-09-05T03:02:21.831463Z"
    },
    "id": "Labwwn3nZnoy",
    "outputId": "63b55df1-d389-45d4-925e-2bf9b40da21b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='jet')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:22.998923Z",
     "iopub.status.busy": "2022-09-05T03:02:22.998098Z",
     "iopub.status.idle": "2022-09-05T03:02:24.463297Z",
     "shell.execute_reply": "2022-09-05T03:02:24.462125Z",
     "shell.execute_reply.started": "2022-09-05T03:02:22.998876Z"
    },
    "id": "hZJQ_FW5azlN",
    "outputId": "c572ba10-870f-4718-fedd-caa7f1b42dbd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='binary')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:24.465423Z",
     "iopub.status.busy": "2022-09-05T03:02:24.464957Z",
     "iopub.status.idle": "2022-09-05T03:02:25.59319Z",
     "shell.execute_reply": "2022-09-05T03:02:25.592124Z",
     "shell.execute_reply.started": "2022-09-05T03:02:24.465381Z"
    },
    "id": "RCtDRXrmazie",
    "outputId": "5d93cb38-a1a7-47e2-b470-c12615ff3855",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='afmhot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:25.595886Z",
     "iopub.status.busy": "2022-09-05T03:02:25.594906Z",
     "iopub.status.idle": "2022-09-05T03:02:26.695234Z",
     "shell.execute_reply": "2022-09-05T03:02:26.69414Z",
     "shell.execute_reply.started": "2022-09-05T03:02:25.595854Z"
    },
    "id": "5BIfKdeRazf_",
    "outputId": "5d0db459-8ba3-4884-c20d-0ef80d5de368",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for i in range(15):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    id = np.random.randint(len(images))\n",
    "    show_mask(images[id], masks[id], cmap='copper')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6XkXzkHHcCp"
   },
   "source": [
    "# **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:26.699313Z",
     "iopub.status.busy": "2022-09-05T03:02:26.698904Z",
     "iopub.status.idle": "2022-09-05T03:02:26.710779Z",
     "shell.execute_reply": "2022-09-05T03:02:26.709289Z",
     "shell.execute_reply.started": "2022-09-05T03:02:26.699276Z"
    },
    "id": "GQksNZycb_Xu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(Layer):\n",
    "    \"\"\"Custom encoder block: conv -> dropout -> conv -> optional max pooling.\"\"\"\n",
    "    def __init__(self, filters, dropout_rate, pooling=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.pooling = pooling\n",
    "        self.c1 = Conv2D(filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.drop = Dropout(dropout_rate)\n",
    "        self.c2 = Conv2D(filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.pool = MaxPool2D()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.c2(x)\n",
    "        return (self.pool(x), x) if self.pooling else x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"filters\": self.filters, \"dropout_rate\": self.dropout_rate, \"pooling\": self.pooling})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:26.712856Z",
     "iopub.status.busy": "2022-09-05T03:02:26.712045Z",
     "iopub.status.idle": "2022-09-05T03:02:26.723339Z",
     "shell.execute_reply": "2022-09-05T03:02:26.722409Z",
     "shell.execute_reply.started": "2022-09-05T03:02:26.712825Z"
    },
    "id": "L_9pZ82jHJsG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(Layer):\n",
    "    \"\"\"Custom decoder block: upsample -> concatenate skip -> encoder block.\"\"\"\n",
    "    def __init__(self, filters, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.up = UpSampling2D()\n",
    "        self.encoder_block = EncoderBlock(filters, dropout_rate, pooling=False)\n",
    "    def call(self, inputs):\n",
    "        x, skip = inputs\n",
    "        x = self.up(x)\n",
    "        x = concatenate([x, skip])\n",
    "        return self.encoder_block(x)\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"filters\": self.filters, \"dropout_rate\": self.dropout_rate})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Attention Gate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:26.725865Z",
     "iopub.status.busy": "2022-09-05T03:02:26.724616Z",
     "iopub.status.idle": "2022-09-05T03:02:26.737361Z",
     "shell.execute_reply": "2022-09-05T03:02:26.736385Z",
     "shell.execute_reply.started": "2022-09-05T03:02:26.725828Z"
    },
    "id": "QG5cCor3JiaS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionGate(Layer):\n",
    "    \"\"\"\n",
    "    Attention gate for UNet: refines skip connections by focusing on relevant features.\n",
    "    Used before skip connection concatenation.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters, use_bn, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.use_bn = use_bn\n",
    "        self.norm = Conv2D(filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.down = Conv2D(filters, 3, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.attn = Conv2D(1, 1, activation=\"sigmoid\")\n",
    "        self.up = UpSampling2D()\n",
    "        self.bn = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, skip = inputs\n",
    "        gating = self.norm(x)\n",
    "        skip_proj = self.down(skip)\n",
    "        attn_coeff = self.attn(gating + skip_proj)\n",
    "        attn_coeff = self.up(attn_coeff)\n",
    "        weighted_skip = Multiply()([attn_coeff, skip])\n",
    "        return self.bn(weighted_skip) if self.use_bn else weighted_skip\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"filters\": self.filters, \"use_bn\": self.use_bn})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visual Callbacks and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:26.741458Z",
     "iopub.status.busy": "2022-09-05T03:02:26.741172Z",
     "iopub.status.idle": "2022-09-05T03:02:26.750009Z",
     "shell.execute_reply": "2022-09-05T03:02:26.748822Z",
     "shell.execute_reply.started": "2022-09-05T03:02:26.741433Z"
    },
    "id": "IsCTVw0tNeZg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ShowProgress(Callback):\n",
    "    \"\"\"\n",
    "    Keras Callback to visualize predictions vs ground truth after every epoch.\n",
    "    Aids in quick qualitative model check during training.\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        idx = np.random.randint(0, len(images))\n",
    "        exp = GradCAM()\n",
    "        image = images[idx]\n",
    "        mask = masks[idx]\n",
    "        pred_mask = self.model.predict(image[np.newaxis, ...])\n",
    "        cam = exp.explain(\n",
    "            validation_data=(image[np.newaxis, ...], mask),\n",
    "            class_index=1,\n",
    "            layer_name='Attention4',\n",
    "            model=self.model)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(\"Original Mask\")\n",
    "        show_mask(image, mask, cmap='copper')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        show_mask(image, pred_mask, cmap='copper')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        show_image(cam, title=\"GradCAM\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Attention UNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:02:26.752223Z",
     "iopub.status.busy": "2022-09-05T03:02:26.751775Z",
     "iopub.status.idle": "2022-09-05T03:02:27.32494Z",
     "shell.execute_reply": "2022-09-05T03:02:27.323986Z",
     "shell.execute_reply.started": "2022-09-05T03:02:26.752189Z"
    },
    "id": "JUqEssMcQ8me",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "input_layer = Input(shape=images.shape[-3:])\n",
    "\n",
    "# Encoder\n",
    "p1, c1 = EncoderBlock(32,0.1, name=\"Encoder1\")(input_layer)\n",
    "p2, c2 = EncoderBlock(64,0.1, name=\"Encoder2\")(p1)\n",
    "p3, c3 = EncoderBlock(128,0.2, name=\"Encoder3\")(p2)\n",
    "p4, c4 = EncoderBlock(256,0.2, name=\"Encoder4\")(p3)\n",
    "\n",
    "# Encoding\n",
    "encoding = EncoderBlock(512,0.3, pooling=False, name=\"Encoding\")(p4)\n",
    "\n",
    "# Attention + Decoder\n",
    "\n",
    "a1 = AttentionGate(256, bn=True, name=\"Attention1\")([encoding, c4])\n",
    "d1 = DecoderBlock(256,0.2, name=\"Decoder1\")([encoding, a1])\n",
    "\n",
    "a2 = AttentionGate(128, bn=True, name=\"Attention2\")([d1, c3])\n",
    "d2 = DecoderBlock(128,0.2, name=\"Decoder2\")([d1, a2])\n",
    "\n",
    "a3 = AttentionGate(64, bn=True, name=\"Attention3\")([d2, c2])\n",
    "d3 = DecoderBlock(64,0.1, name=\"Decoder3\")([d2, a3])\n",
    "\n",
    "\n",
    "a4 = AttentionGate(32, bn=True, name=\"Attention4\")([d3, c1])\n",
    "d4 = DecoderBlock(32,0.1, name=\"Decoder4\")([d3, a4])\n",
    "\n",
    "# Output \n",
    "output_layer = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(d4)\n",
    "\n",
    "# Model\n",
    "model = Model(\n",
    "    inputs=[input_layer],\n",
    "    outputs=[output_layer]\n",
    ")\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', MeanIoU(num_classes=2, name='IoU')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "cb = [\n",
    "    # EarlyStopping(patience=3, restore_best_weight=True), # With Segmentation I trust on eyes rather than on metrics\n",
    "    ModelCheckpoint(\"AttentionCustomUNet.h5\", save_best_only=True),\n",
    "    ShowProgress()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:13:21.538381Z",
     "iopub.status.busy": "2022-09-05T03:13:21.53732Z",
     "iopub.status.idle": "2022-09-05T03:17:44.983339Z",
     "shell.execute_reply": "2022-09-05T03:17:44.98227Z",
     "shell.execute_reply.started": "2022-09-05T03:13:21.538333Z"
    },
    "id": "MpBP0XH6RJEY",
    "outputId": "2a4bf608-8f89-4c13-81f8-b1ee04e8a856",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Config Training\n",
    "BATCH_SIZE = 8\n",
    "SPE = len(images)//BATCH_SIZE\n",
    "\n",
    "# Training\n",
    "results = model.fit(\n",
    "    images, masks,\n",
    "    validation_split=0.2,\n",
    "    epochs=20, # 15 will be enough for a good Model for better model go with 20+\n",
    "    steps_per_epoch=SPE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=cb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nJgacMfUMro"
   },
   "source": [
    "Observations:\n",
    "\n",
    "Around the 12th epoch, the model started outputting meaningful segmentation results.\n",
    "\n",
    "The model is particularly good at detecting well-defined, black round spots. However, as is typical with many segmentation models, it still struggles when the shapes become irregular—although this is improved in the current run due to the use of a high steps-per-epoch (SPE) setting.\n",
    "\n",
    "Confusions often arise in dark, ambiguous image regions, which is understandable both biologically and technically since areas of similar intensity can fool even human observers.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "Chunking training into intervals of 20 epochs (e.g., train in blocks of 20) gives you better control over progress and helps to optimize model performance over time.\n",
    "\n",
    "In this experiment, the model was trained for three blocks of 17 epochs (totaling 51 epochs), a strategy that proved effective.\n",
    "\n",
    "Even when inspecting outputs on tougher images, most failures (about 9 out of 10) occur on cases that would challenge expert human readers as well—because in those images, key features are genuinely ambiguous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:17:44.989069Z",
     "iopub.status.busy": "2022-09-05T03:17:44.988759Z",
     "iopub.status.idle": "2022-09-05T03:17:44.994365Z",
     "shell.execute_reply": "2022-09-05T03:17:44.993398Z",
     "shell.execute_reply.started": "2022-09-05T03:17:44.989041Z"
    },
    "id": "m5uV8wgMWlao",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy, iou, val_loss, val_accuracy, val_iou = results.history.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:17:44.996888Z",
     "iopub.status.busy": "2022-09-05T03:17:44.99582Z",
     "iopub.status.idle": "2022-09-05T03:17:45.494628Z",
     "shell.execute_reply": "2022-09-05T03:17:45.49377Z",
     "shell.execute_reply.started": "2022-09-05T03:17:44.99685Z"
    },
    "id": "KUgo9t-bW2aH",
    "outputId": "e7672a19-48bc-407e-c4c9-c277c253a730",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(\"Model Loss\")\n",
    "plt.plot(loss, label=\"Training\")\n",
    "plt.plot(val_loss, label=\"Validtion\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.plot(accuracy, label=\"Training\")\n",
    "plt.plot(val_accuracy, label=\"Validtion\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"Model IoU\")\n",
    "plt.plot(iou, label=\"Training\")\n",
    "plt.plot(val_iou, label=\"Validtion\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfhLm2LOXvOw"
   },
   "source": [
    "The evaluation has revealed an unusual trend: the IoU (Intersection over Union) score on the validation data is significantly higher than on the training data. Generally, we'd expect the model to fit the training set better (or even overfit), so this reversal is intriguing.\n",
    "\n",
    "Possible explanations include:\n",
    "\n",
    "The validation set could be inadvertently “easier” or better aligned to the model’s learned features.\n",
    "\n",
    "Some regularization or randomness may help the model generalize on the validation split more effectively.\n",
    "\n",
    "There may have been fluctuations in training, which could be tied to factors like learning rate or split randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:19:30.272136Z",
     "iopub.status.busy": "2022-09-05T03:19:30.271081Z",
     "iopub.status.idle": "2022-09-05T03:19:32.385401Z",
     "shell.execute_reply": "2022-09-05T03:19:32.384429Z",
     "shell.execute_reply.started": "2022-09-05T03:19:30.272095Z"
    },
    "id": "fvjqCPIMW2XG",
    "outputId": "070d77c1-5f7a-4261-f74f-9b70484f58fc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,25))\n",
    "n=0\n",
    "for i in range(1,(5*3)+1):\n",
    "    plt.subplot(5,3,i)\n",
    "    if n==0:\n",
    "        id = np.random.randint(len(images))\n",
    "        image = images[id]\n",
    "        mask = masks[id]\n",
    "        pred_mask = model.predict(image[np.newaxis,...])\n",
    "\n",
    "        plt.title(\"Original Mask\")\n",
    "        show_mask(image, mask)\n",
    "        n+=1\n",
    "    elif n==1:\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        show_mask(image, pred_mask)\n",
    "        n+=1\n",
    "    elif n==2:\n",
    "        pred_mask = (pred_mask>0.5).astype('float')\n",
    "        plt.title(\"Processed Mask\")\n",
    "        show_mask(image, pred_mask)\n",
    "        n=0\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are promising,\n",
    "\n",
    "Going forward, possible improvements could include:\n",
    "\n",
    "Exploring more advanced augmentation or processing strategies to maximize generalization.\n",
    "\n",
    "Experimenting with other attention mechanisms or hybrid architectures for further accuracy.\n",
    "\n",
    "Using more robust cross-validation and model interpretability techniques for greater transparency."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1209633,
     "sourceId": 2021025,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2456391,
     "sourceId": 4161522,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30235,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
